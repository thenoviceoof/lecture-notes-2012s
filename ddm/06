DDM
Lecture #6
2012/03/05
================================================================================

Review
max likelihood linear classifiers (generalize on least squares)
  linear regression
    L = \sum_i^N (y_i - w\cdot x_i)^2
    includes X^T X -> (k by k) size
    // we're going to go to (n by n) today
    gradient descent - iterative process
    squared loss penalizes correct outliers disproportionally
  logistic regression
    p(y=1|x) = 1/(1 + e^{-w\cdot x})
    L = \prod p_i^y_i (1-p_i)^(1-y_i)
    taking the derivative results in a trancendental function
    have to do gradient descent

Overview
  regularization
  non-linear classifiers
    kernel trap

--------------------------------------------------------------------------------
Regularization

Infinite computing power
Why is X^T X is still bad?
  if you have more features than examples, X^T X is rank deficient,
  and you can't invert it. if you have enough examples, you're just
  memorizing things anyways
We should be able to do better

penalize parameters for being too large
  L_D(w) + \lambda J(w)
  // where L_D(w) dep on data
  // J(w) is data indep (turns out to be ||w||^2)
  // \lambda is chosen: parameter weighing effect of data
  Ridge inference (type of inference, inc lasso, etc)
why?
  motivation: max a-posteriori
  likelihood:
    p(D|w) = \prod_i^N N(y; yhat(x_i;w), \sigma)
  prior:
    p(w) = N(w; 0, \lambda^-1)
  MAP (max a-posteriori):
    argmax_w p(w|D) = argmax_w p(D|w)p(w)
    log p(w|D) = 1/(2\sigma^2) \sum (y_i - yhat_i)^2 + \lambda/2 ||w||^2
    // remember yhat_i = [Xw]_i
    d_w(L) = -X^T[y-X-w] + \lambda w
    X^T[y-X-w] = \lambda w
    w = (X^T X + \lambda I)^-1 X^T y
    // adding lambda to X^T X on the diagonal means we have full rank
    choose \lambda by cross-validation

gradient descent with regularization
w = w - \eta d_w(L)
  // d_w(L) = -X^T [y-Xw] + \lambda w
  = (1-\eta\lambda)w - \eta X^T (y - Xw)
  // effectively, it tries to minimize w: tada! params penalized
  // have to tune \eta, though
// linear regression, calculate the full posterior, you get the
// bayesian occam's razor. hmm

--------------------------------------------------------------------------------
non-linear classifiers

linear:
y(x; w) = f(w\cdot x)
  always slicing a hyperplane of sorts

you can't always slice it with a hyperplane
do a space transformation (\phi)
maybe x_1^2 vs. x_2^2, if you have 2d data in a circle
  linear division maps to nonlinear boundary in original space

but you don't always know how to transform your space
example:
  all pairwise features from original k
  k + \choose{k}{2} ~ O(k^2)
we'll assume we have enough memory to do these things (the transform)
but eventually move on to the kernel trick, skipping the transform

yhat = w\cdot x -> w \cdot \phi(x)
     = Xw -> \Phi w
w = (X^T X + \lambda)^-1 X^T y -> (\Phi^t\Phi + \lambda)^-1 \Phi^T y
 X_ik is kth feature, ith example
 \Phi_ik' is kth element of \phi applied to ith example
   [\phi(x_i)]_k'

but we can do better!

----------------------------------------
Kernelized Ridge Regression (kernel trick)

loss function:
L = 1/2 \sum_i (y_i - w\cdot \phi(x_i))^2 + \lambda/2 ||w||^2
d_w(L) = -\sum_i (y_i - w\cdot \phi(x_i))\phi(x_i) + \lambda w
w = 1/\lambda \sum_i (y_i - w\cdot \phi(x_i))\phi(x_i)
a_i = 1/lambda (y_i - w\cdot \phi(x_i))
w = \sum_i a_i \phi(x_i)
  = \Phi^T a
a = 1/lambda (y_i - \Phi w)
  = 1/lambda (y_i - \Phi\Phi^T a)
\lambda a = y - \Phi\Phi^T a
a = (\Phi\Phi^T + \lambda)^-1 y
w = \Phi^T(\Phi\Phi^T + \lambda)^-1 y
  // it's now \Phi\Phi^T -> size (n,n), not (k,k)

add new points X~ -> \phi(x~) => \Phi~
yhat = \Phi~ w = \Phi~ \Phi^T (\Phi\Phi^T + \lambda)^-1 y
               = K  // gram matrix
K_ij = K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
\Phi~ = K(x_i~, x_j)

yay, a kernel!

example:
Pairwise features
2-dimensional space, x = (x_1, x_2) and z = (z_1, z_2)
k(x,z) = (x\cdot z)^2
       = (x_1 z_1 + x_2 z_2)^2
       = x_1^2 z_1^2 + 2x_1 x_2 z_1 z_2 + x_2^2 z_2^2
       = (x_1^2, \sqrt{2} x_1x_2, x_2^2) \cdot (z_1^2, \sqrt{2} z_1z_2, z_2^2)
       = \phi(x) \cdot \phi(z)

K needs to be a positive semi-definite matrix for any x
can apply a bunch of operations to a kernel thing, see bishop

also, this method tries to minimize a(?), but non-zero

talking about support vector machines
"sparse kernel machine"
only a few points matter in finding the margin, so only their a_i != 0

end 1st part of class